<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Projects | Maryam Sana</title>
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            max-width: 900px;
            margin: 40px auto;
            padding: 0 20px;
            line-height: 1.65;
            color: #222;
        }
        nav {
            margin-bottom: 30px;
        }
        nav a {
            margin-right: 18px;
            text-decoration: none;
            color: #0066cc;
            font-weight: 500;
        }
        nav a:hover {
            text-decoration: underline;
        }
        h1 {
            margin-bottom: 10px;
        }
        h2 {
            margin-top: 50px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 6px;
        }
        h3 {
            margin-top: 25px;
        }
        img {
            max-width: 100%;
            margin: 20px 0;
            border: 1px solid #ddd;
            padding: 5px;
            background: #fafafa;
        }
        ul {
            margin-left: 20px;
        }
        .status {
            font-style: italic;
            color: #555;
        }
    </style>
</head>
<body>

<nav>
    <a href="index.html">Home</a>
    <a href="projects.html">Projects</a>
    <a href="awards.html">Awards & Funding</a>
</nav>

<h1>Research Projects</h1>
<p>
    My research focuses on generative modeling, perception, and decision-making under
    severe information and resource constraints, spanning video, neural signals, and
    embedded vision systems.
</p>

<!-- ================= PROJECT 1 ================= -->

<section>
    <h2>Perceptual Video Compression with Diffusion Models</h2>
    <p class="status">Status: Completed (Master’s Research)</p>

    <h3>Problem</h3>
    <p>
        Ultra-low bitrate (ULB) video delivery in the range of <strong>0.005–0.02 bpp</strong>
        remains largely unsolved. Classical codecs such as H.264 and HEVC optimize for
        pixel-level distortion and exhibit severe perceptual degradation at these rates.
    </p>

    <h3>Method</h3>
    <p>
        I developed the first diffusion-based learned video codec capable of operating
        effectively in this regime. The encoder transmits only sparse, high-level semantic
        priors—latent keyframes and lightweight motion trajectories—while a controllable
        diffusion model at the decoder synthesizes temporally coherent intermediate frames.
    </p>

    <h3>Architecture</h3>
    <img src="projects/figs/Encoder.png" alt="Encoder Architecture">
    <img src="projects/figs/Decoder.png" alt="Decoder Architecture">

    <h3>Key Contributions</h3>
    <ul>
        <li>Semantic-guided generative video codec operating below 0.02 bpp.</li>
        <li>Trajectory-aware synthesis improving temporal coherence and perceptual quality.</li>
        <li>Comprehensive benchmarking against H.264, HEVC/VVC, and learned codecs.</li>
    </ul>
</section>

<!-- ================= PROJECT 2 ================= -->

<section>
    <h2>EEG → Image Reconstruction with Generative Models</h2>
    <p class="status">Status: Ongoing</p>

    <h3>Motivation</h3>
    <p>
        This project investigates how visual information is represented and can be
        reconstructed from neural signals under extreme information constraints.
        The goal is to bridge computational neuroscience and generative modeling.
    </p>

    <h3>Approach</h3>
    <p>
        I work with EEG and fMRI data using established neuroscience toolkits (EEGLAB, MNE)
        and large-scale public datasets such as THINGS and NSD. Reconstruction is framed as
        a probabilistic inference problem, explicitly modeling uncertainty, noise, and
        information bottlenecks.
    </p>

    <p>
        Current work focuses on diffusion-based EEG-to-image reconstruction pipelines and
        prototyping real-time systems that address latency, robustness, and entropy
        constraints inherent to neural signals.
    </p>

    <!-- <h3>Architecture (Work in Progress)</h3>
    <img src="eeg_architecture.jpg" alt="EEG to Image Reconstruction Architecture"> -->
</section>

<!-- ================= PROJECT 3 ================= -->

<section>
    <h2>Real-Time Disaster Management (RTDM) on Embedded UAV Platforms</h2>
    <p class="status">Status: Completed (Undergraduate Research)</p>

    <h3>Problem</h3>
    <p>
        Disaster response scenarios require rapid situational awareness under strict
        constraints on computation, power, and latency. UAV-based systems must operate
        fully on-board while performing accurate disaster detection and victim localization.
    </p>

    <h3>System Overview</h3>
    <p>
        I designed a Real-Time Disaster Management (RTDM) module that sequentially integrates
        quantized image classification and object detection models. The system processes
        live UAV camera feeds and communicates actionable insights to a ground control station.
    </p>

    <img src="projects/figs/RTDM_final.png" alt="RTDM Module Overview">

    <h3>Key Contributions</h3>
    <ul>
        <li>
            Designed <strong>Squeeze-ErNet</strong>, a quantized architecture achieving
            ~94.4% accuracy at ~510 FPS on NVIDIA Jetson TX2.
        </li>
        <li>
            Systematic evaluation of quantization schemes for performance–accuracy trade-offs.
        </li>
        <li>
            Created the <strong>ODDER</strong> dataset for aerial emergency response
            (2 classes, 1400+ images).
        </li>
        <li>
            Deployment and power analysis on embedded UAV hardware.
        </li>
    </ul>
</section>

</body>
</html>
